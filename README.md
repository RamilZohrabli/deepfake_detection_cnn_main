# Deepfake_detection_cnn
Abstract— This paper systematically analyzes three lightweight CNN architectures—MobileNetV3-Small, EfficientNet-B0, and ResNet18—for real-time deepfake detection under a unified preprocessing, training, and evaluation pipeline. Beyond reporting numerical differences in accuracy, precision, recall, F1-score, and throughput, we explicitly investigate architectural factors that shape model behavior, including residual connections, depthwise-separable convolutions, squeeze-and-excite blocks, compound scaling, and the effective number of trainable parameters exposed during transfer learning. On the Google DeepFake Detection (DFD) dataset, all models are fine-tuned from ImageNet and benchmarked on an NVIDIA RTX GPU. ResNet18 achieves the strongest overall performance (92.5% accuracy, 92.8% F1-score, 97.7% recall for fake frames) with moderate latency (~0.56 ms/frame, 1801 FPS), while MobileNetV3-Small attains a competitive F1-score (90.4%) and offers the best speed–accuracy trade-off (~0.46 ms/frame, 2223 FPS). In contrast, EfficientNet-B0—despite having the largest total parameter count—obtains the lowest accuracy (87.8% with 87.97? F1-score) and the slowest inference (~1.20 ms/frame, 880 FPS), largely because only 0.06% of its weights are trainable in our setup, causing the backbone to act as an almost frozen feature extractor. Through joint analysis of confusion matrices, learning curves, parameter statistics, and latency, we show that the interaction between architectural design and trainable capacity is a stronger predictor of deepfake detection performance than raw model size alone. The same methodology is later extended to the Celeb-DF dataset to examine how these conclusions generalize to more challenging, high-quality forgeries and to derive practical guidelines for selecting lightweight CNN backbones in real-time deepfake detection systems.
